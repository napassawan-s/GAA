# -*- coding: utf-8 -*-
"""GAA-Model-Cross.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z7rGCqDpGfqIsTeZ5qjnCq8jhnOasy9Z
"""

import pandas as pd
import numpy as np
import logging
import tensorflow as tf
import warnings
import glob
import tqdm
import os

from tqdm import tqdm 
from IPython import display 
import matplotlib.pyplot as plt
import seaborn as sns
from seaborn import heatmap

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import KFold

from skimage.io import imread, imshow
from skimage.transform import resize

from keras.models import Sequential, load_model, save_model
from keras.layers import Conv2D, Lambda, MaxPooling2D, Dense, Dropout, Flatten # convolution layers & core layers

from tensorflow.keras.layers import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical

from tensorflow import keras
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History
  
base_dir = '.\\AfterConvert'
train_dir = os.path.join(base_dir, 'Train')
test_dir = os.path.join(base_dir, 'Test')

train_g = glob.glob(os.path.join(train_dir, 'Glass', '*.jpg'))+glob.glob(os.path.join(train_dir, 'Glass', '*.HEIC'))+glob.glob(os.path.join(train_dir, 'Glass', '*.heic'))+glob.glob(os.path.join(train_dir, 'Glass', '*.JPG'))
train_m = glob.glob(os.path.join(train_dir, 'Metal', '*.jpg'))+glob.glob(os.path.join(train_dir, 'Metal', '*.HEIC'))+glob.glob(os.path.join(train_dir, 'Metal', '*.heic'))+glob.glob(os.path.join(train_dir, 'Metal', '*.JPG'))
train_pa = glob.glob(os.path.join(train_dir, 'Paper', '*.jpg'))+glob.glob(os.path.join(train_dir, 'Paper', '*.HEIC'))+glob.glob(os.path.join(train_dir, 'Paper', '*.heic'))+glob.glob(os.path.join(train_dir, 'Paper', '*.JPG'))
train_pl = glob.glob(os.path.join(train_dir, 'Plastic', '*.jpg'))+glob.glob(os.path.join(train_dir, 'Plastic', '*.HEIC'))+glob.glob(os.path.join(train_dir, 'Plastic', '*.heic'))+glob.glob(os.path.join(train_dir, 'Plastic', '*.JPG'))

a = len(train_pa)
b = len(train_pl)
c = len(train_g)
d = len(train_m)

print("Number of training samples: {}".format(a+b+c+d))

test_g = glob.glob(os.path.join(test_dir, 'Glass', '*.jpg'))+glob.glob(os.path.join(test_dir, 'Glass', '*.HEIC'))+glob.glob(os.path.join(test_dir, 'Glass', '*.heic'))+glob.glob(os.path.join(test_dir, 'Glass', '*.JPG'))
test_m = glob.glob(os.path.join(test_dir, 'Metal', '*.jpg'))+glob.glob(os.path.join(test_dir, 'Metal', '*.HEIC'))+glob.glob(os.path.join(test_dir, 'Metal', '*.heic'))+glob.glob(os.path.join(test_dir, 'Metal', '*.JPG'))
test_pa = glob.glob(os.path.join(test_dir, 'Paper', '*.jpg'))+glob.glob(os.path.join(test_dir, 'Paper', '*.HEIC'))+glob.glob(os.path.join(test_dir, 'Paper', '*.heic'))+glob.glob(os.path.join(test_dir, 'Paper', '*.JPG'))
test_pl = glob.glob(os.path.join(test_dir, 'Plastic', '*.jpg'))+glob.glob(os.path.join(test_dir, 'Plastic', '*.HEIC'))+glob.glob(os.path.join(test_dir, 'Plastic', '*.heic'))+glob.glob(os.path.join(test_dir, 'Plastic', '*.JPG'))

a = len(test_pa)
b = len(test_pl)
c = len(test_g)
d = len(test_m)

print(test_g)
print("Number of testing samples: {}".format(a+b+c+d))

train_datagen = ImageDataGenerator(rescale = 1.0 / 255.0,
                                   zoom_range = 0.4,
                                   rotation_range = 10,
                                   horizontal_flip = True,
                                   vertical_flip = True,
                                   validation_split = 0.2)

valid_datagen = ImageDataGenerator(rescale = 1.0 / 255.0,
                                   validation_split = 0.2)

test_datagen  = ImageDataGenerator(rescale = 1.0 / 255.0)

train_ds  = train_datagen.flow_from_directory(directory = train_dir,
                                                   target_size = (224, 224),
                                                   class_mode = 'categorical',
                                                   batch_size = 32, 
                                                   subset = 'training')

valid_ds = valid_datagen.flow_from_directory(directory = train_dir,
                                                  target_size = (224, 224),
                                                  class_mode = 'categorical',
                                                  batch_size = 32, 
                                                  subset = 'validation')

test_ds = test_datagen.flow_from_directory(directory = test_dir,
                                                  target_size = (224, 224),
                                                  class_mode = 'categorical',
                                                  batch_size = 32,
                                                  shuffle=False)

len(train_ds.classes)

len(train_ds.filenames)

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.losses import sparse_categorical_crossentropy, categorical_crossentropy
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import KFold
import numpy as np

# Model configuration
batch_size = 32
img_width, img_height, img_num_channels = 224, 224, 3
loss_function = categorical_crossentropy
no_classes = 4
no_epochs = 1
verbosity = 1
num_folds = 2

# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []
val_acc_per_fold = []
val_loss_per_fold = []

# Define the K-fold Cross Validator
kfold = KFold(n_splits=num_folds, shuffle=True)

filepath = './model_cross_sgd_real.hdf5'
filepath_final = './model_cross_sgd_final.hdf5'

earlystopping = EarlyStopping(monitor = 'accuracy', 
                              mode = 'max' , 
                              patience = 50,
                              verbose = 1)

checkpoint = ModelCheckpoint(filepath, 
                                monitor = 'accuracy', 
                                mode='max', 
                                save_best_only=True, 
                                verbose = 1)


callback_list = [earlystopping, checkpoint]

# K-fold Cross Validation model evaluation
fold_no = 1
for train in kfold.split(train_ds.filenames, train_ds.classes):

  # Define the model architecture
  base_model = VGG16(input_shape=(224,224,3), 
                   include_top=False,
                   weights="imagenet")
  for layer in base_model.layers:
    layer.trainable=False
  
  # Defining Layers
    model=Sequential()
    model.add(base_model) 
    model.add(Dropout(0.2))
    model.add(Flatten())

# Add dense layers
    model.add(BatchNormalization())
    model.add(Dense(5000,activation="relu",kernel_initializer='he_uniform'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(1000,activation="relu",kernel_initializer='he_uniform'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(500,activation="relu",kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(4,activation="softmax"))

    # Compile the model
  opt = tf.keras.optimizers.SGD(learning_rate=0.01)
  model.compile(loss=categorical_crossentropy,
                optimizer=opt,
                metrics=['accuracy'])


  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  # Fit data to model
  history = model.fit(train_ds,
              batch_size=batch_size,
              epochs=no_epochs,
              validation_data=valid_ds,
              callbacks = callback_list,
              verbose=1)

  # Generate generalization metrics
  scores = model.evaluate(train_ds)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
  print('score' ,scores)
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])
  

  # Increase fold number
  fold_no = fold_no + 1

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

history_data = history
save_model(model, filepath_final)


# Save as DataFrame:     
history_df = pd.DataFrame(history_data.history) 
history_df
history_df.to_csv('model/model_history_cross_sgd_real.csv', index=False)

plt.figure(figsize=(12,7))
plt.plot(history.history['accuracy'], color='deeppink', linewidth=4)
plt.plot(history.history['val_accuracy'], color='dodgerblue', linewidth=4)
plt.title('Model Accuracy SGD', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch', fontsize=14, fontweight='bold')
plt.legend(['Train', 'Validation'], loc='upper left', bbox_to_anchor=(1,1), fontsize=14)
plt.show()

plt.figure(figsize=(12,7))
plt.plot(history.history['loss'], color='deeppink', linewidth=4)
plt.plot(history.history['val_loss'], color='dodgerblue', linewidth=4)
plt.title('Model Loss SGD', fontsize=14, fontweight='bold')
plt.ylabel('Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch', fontsize=14, fontweight='bold')
plt.legend(['Train', 'Validation'], loc='upper left', bbox_to_anchor=(1,1), fontsize=14)
plt.show()